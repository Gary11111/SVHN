{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一些超参\n",
    "minibatch_size = 64  # the number of instances in a batch ：一个batch的图片数量\n",
    "nr_channel = 3 # the channels of image\n",
    "image_shape = (32, 32) # the image shape (height, width)\n",
    "nr_class = 10 # the number of classes\n",
    "nr_epoch = 20 # the max epoch of training 控制整个数据集通过神经网络的次数\n",
    "weight_decay = 1e-10 # a strength of regularization\n",
    "test_interval = 5 # test in every ${test_interval} epochs\n",
    "show_interval = 10 # print a message of training in every ${show_interval} minibatchs \n",
    "\n",
    "trainSize = 73257 # 73257->30000->10000\n",
    "testSize = 26032 # 26032 constant\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import io as scio\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "# %matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorInv(img): # colore inversion\n",
    "    for k in range(3):\n",
    "        for i in range(32):\n",
    "            for j in range(32):\n",
    "                flag = np.random.exponential(10, size=1)  # 产生1个满足指数分布的随机数\n",
    "                if flag < 0.01:\n",
    "                    img[i,j] = 255 - img[i,j]\n",
    "    return img\n",
    "\n",
    "def addsalt_pepper(img, SNR = 0.7):\n",
    "    img_ = img.transpose(2,1,0)\n",
    "    c, h, w = img_.shape\n",
    "    mask = np.random.choice((0, 1, 2), size=(1, h, w), p=[SNR, (1 - SNR) / 2., (1 - SNR) / 2.])\n",
    "    mask = np.repeat(mask, c, axis=0)     # 按channel 复制到 与img具有相同的shape\n",
    "    img_[mask == 1] = 255    # pepper\n",
    "    img_[mask == 2] = 0      # white\n",
    "    return img_.transpose(2,1,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建TF迭代器\n",
    "- reduce the amount of images labelled with'8' '9' and '0' to 500 and get Dataset A\n",
    "\n",
    "  cnt = [500, 13861, 10585, 8497, 7458, 6882, 5727, 5595, 500, 500]\n",
    "\n",
    "- reduce the amount of images labelled with '6', '7', '8','9' and '0' to 1000 and get Dataset B\n",
    "\n",
    "  cnt = [1000, 13861, 10585, 8497, 7458, 6882, 1000, 1000, 1000, 1000]\n",
    "\n",
    "- reduce the amount of images labelled with '1','2','3','4'and '5' to 6000 and get Dataset C\n",
    "\n",
    "  cnt = [4948, 6000, 6000, 6000, 6000, 6000, 5727, 5595, 5045, 4659]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dataset():\n",
    "    dsPath = './data' # path for saving dataset\n",
    "    dsMeta = {\n",
    "        #'train':('./obj/train_color.obj',trainSize),\n",
    "        'train':('./obj/train_sp.obj',trainSize),\n",
    "        # 'train': ([os.path.join(dsPath,'train_32x32.mat')],trainSize),\n",
    "        'test':([os.path.join(dsPath,'test_32x32.mat')],testSize),\n",
    "    }\n",
    "    \n",
    "    def __init__(self,dsName):\n",
    "        self.files, self.instances = self.dsMeta[dsName] # 定义files 和 instance 成员\n",
    "    \n",
    "    def load(self):\n",
    "        datas_list, labels_list = [], []\n",
    "        print(self.files)\n",
    "        if self.files == './obj/train_sp.obj':\n",
    "            f = open(self.files,'rb')\n",
    "            data = pickle.load(f)\n",
    "            data['X'] = data['X'].transpose(0,3,1,2)\n",
    "            self.samples = {\n",
    "                'X':data['X'],\n",
    "                'Y':data['y'],\n",
    "            }\n",
    "        else :\n",
    "            for f in self.files:\n",
    "                samples = scio.loadmat(f) # 使用load函数解压.mat文件\n",
    "                datas_list.append(samples['X'])\n",
    "                labels_list.append(samples['y'])\n",
    "                # print(samples['X'].shape,samples['X'])\n",
    "                # break\n",
    "            self.samples = {\n",
    "                'X': np.concatenate(datas_list, axis=3), # datas\n",
    "                'Y': np.concatenate(labels_list, axis=0), # labels\n",
    "            }\n",
    "        return self\n",
    "        \n",
    "    def instance_generator(self): # 产生一张图片\n",
    "        '''a generator to yield a sample'''\n",
    "        cnt0,cnt8,cnt9 = 0,0,0 # dist-A 500\n",
    "        cnt6,cnt7,cnt8,cnt9=0,0,0,0 # dist-B 1000\n",
    "        cnt1, cnt2, cnt3, cnt4, cnt5 =0,0,0,0,0 # dist-C 6000\n",
    "        for i in range(self.instances):\n",
    "            img = self.samples['X'][:, :, :, i]\n",
    "            label = self.samples['Y'][i, :][0]\n",
    "            '''\"控制数据的分布\"\n",
    "            if label == 10:\n",
    "                label = 0 # 把标签为10改为标签为0\n",
    "                cnt0 += 1\n",
    "            elif label == 6:\n",
    "                cnt6 += 1\n",
    "            elif label == 7:\n",
    "                cnt7 += 1\n",
    "            elif label == 8:\n",
    "                cnt8 += 1\n",
    "            elif label == 9:\n",
    "                cnt9 += 1\n",
    "            if (cnt0>=1000 and label==0) or (cnt6 >= 1000 and label==6) or (cnt7 >=1000 and label==7) or (cnt8 >= 1000 and label==8) or (cnt9 >=1000 and label==9):\n",
    "                continue'''\n",
    "            img = cv2.resize(img, image_shape) # 图片统一乘32*32\n",
    "            yield img.astype(np.float32), np.array(label, dtype=np.int32) # 解析出一张图片和label\n",
    "            \n",
    "    @property\n",
    "    def instances_per_epoch(self):\n",
    "        return 25600 # set for a fast experiment\n",
    "        #return self.instances\n",
    "    \n",
    "    @property\n",
    "    def minibatchs_per_epoch(self):\n",
    "        return 200 # set for a fast experimetn\n",
    "        #return self.instances // minibatch_size   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "train = Dataset('train').load()\n",
    "train_sp = []\n",
    "\n",
    "cnt = 0 \n",
    "for i in range(train.instances):\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    img = train.samples['X'][:,:,:,10]\n",
    "    img = addsalt_pepper(img) # 颜色反转\n",
    "    train_sp.append(img)\n",
    "    \n",
    "train_sp = np.array(train_sp)\n",
    "train_sp = train_color.transpose(1,2,3,0)\n",
    "print(train_color.shape)\n",
    "\n",
    "f = open('obj/train_sp.obj','wb')\n",
    "train_sp = {\n",
    "    'X':train_sp,\n",
    "    'y':train.samples['Y'],\n",
    "}\n",
    "pickle.dump(train_sp,f)\n",
    "\n",
    "f = open('./obj/train_sp.obj','rb')\n",
    "print(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put Label Onto IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def showGrid():\n",
    "    print(\"Loading...\")\n",
    "    ds = Dataset('train').load()\n",
    "    ds_gen = ds.instance_generator()\n",
    "    print(\"Finish\")\n",
    "\n",
    "\n",
    "    imggrid = [] # (25*32*32*3)\n",
    "    for i in range(25):\n",
    "        img,label = next(ds_gen)\n",
    "        cv2.putText(img,str(label),(0,32),cv2.FONT_HERSHEY_SIMPLEX,0.6,(0,0,255),2)\n",
    "        imggrid.append(img)\n",
    "\n",
    "    print(imggrid[0].shape)\n",
    "    print(img.shape)\n",
    "    imggrid = np.array(imggrid).reshape((5,5,img.shape[0],img.shape[1],img.shape[2])) # 5*5*32*32*3\n",
    "    imggrid = imggrid.transpose((0,2,1,3,4)).reshape((5*img.shape[0],5*img.shape[1],3)) # \n",
    "    imggrid = cv2.cvtColor(imggrid.astype(\"uint8\"),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(imggrid)\n",
    "    plt.show()\n",
    "    \n",
    "# showGrid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./obj/train_sp.obj\n",
      "[ 4948 13861 10585  8497  7458  6882  5727  5595  5045  4659]\n",
      "73257\n"
     ]
    }
   ],
   "source": [
    "def statistic():\n",
    "    ds = Dataset('train').load()\n",
    "    ds_gen = ds.instance_generator()\n",
    "    labels = np.zeros((10,),dtype=np.int32)\n",
    "    cnt = 0\n",
    "    for img,label in ds_gen:\n",
    "        labels[label % 10] += 1\n",
    "    print(labels)\n",
    "    print(np.sum(labels))\n",
    "    # img = train.samples['X'][:,:,:,10]\n",
    "    # img = addsalt_pepper(img) # 颜色反转\n",
    "    # train_sp.append(img)\n",
    "statistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP NET BUILD\n",
    "控制正则化方法\n",
    "池化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tf_contrib\n",
    "\n",
    "\n",
    "\n",
    "class Model():\n",
    "    '''a class for building a compute graph'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        # set the initializer of conv_weight and conv_bias\n",
    "        self.weight_init = tf_contrib.layers.variance_scaling_initializer(factor=1.0,\n",
    "                                mode='FAN_IN', uniform=False)\n",
    "        self.bias_init = tf.zeros_initializer()\n",
    "        # set l2 regularizer\n",
    "        self.reg = tf_contrib.layers.l2_regularizer(weight_decay)\n",
    "        # set l1 regularizer\n",
    "        #self.reg = tf_contrib.layers.l1_regularizer(weight_decay)\n",
    "\n",
    "    def _conv_layer(self, name, inp, kernel_shape, stride, padding='SAME',is_training=False):\n",
    "        '''a conv layer = conv + bn + relu'''\n",
    "        \n",
    "        with tf.variable_scope(name) as scope:\n",
    "            conv_filter = tf.get_variable(name='filter', shape=kernel_shape,\n",
    "                                          initializer=self.weight_init, regularizer=self.reg)\n",
    "            conv_bias = tf.get_variable(name='bias', shape=kernel_shape[-1],\n",
    "                                        initializer=self.bias_init)\n",
    "            x = tf.nn.conv2d(inp, conv_filter, strides=[1, stride, stride, 1],\n",
    "                             padding=padding, data_format='NHWC')\n",
    "            x = tf.nn.bias_add(x, conv_bias, data_format='NHWC')\n",
    "            x = tf.layers.batch_normalization(x, axis=3, training=is_training)\n",
    "            x = tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "    def _pool_layer(self, name, inp, ksize, stride, padding='SAME', mode='MAX'):\n",
    "        '''a pool layer which only supports avg_pooling and max_pooling(default)'''\n",
    "        \n",
    "        assert mode in ['MAX', 'AVG'], 'the mode of pool must be MAX or AVG'\n",
    "        if mode == 'MAX':\n",
    "            x = tf.nn.max_pool(inp, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1],\n",
    "                               padding=padding, name=name, data_format='NHWC')\n",
    "        elif mode == 'AVG':\n",
    "            x = tf.nn.avg_pool(inp, ksize=[1, ksize, ksize, 1], strides=[1, stride, stride, 1],\n",
    "                               padding=padding, name=name, data_format='NHWC')\n",
    "        return x\n",
    "\n",
    "    def _fc_layer(self, name, inp, units, dropout=0.5):\n",
    "        '''a full connect layer'''\n",
    "        \n",
    "        with tf.variable_scope(name) as scope:\n",
    "            shape = inp.get_shape().as_list() # get the shape of input\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(inp, [-1, dim]) # flatten : 将数据压平：多维的数据归一化\n",
    "            if dropout > 0: # if with dropout\n",
    "                x = tf.nn.dropout(x, keep_prob=dropout, name='dropout')\n",
    "            x = tf.layers.dense(x, units, kernel_initializer=self.weight_init,\n",
    "                                bias_initializer=self.bias_init, kernel_regularizer=self.reg)\n",
    "        return x\n",
    "\n",
    "    def build(self):\n",
    "        # set inputs\n",
    "        data = tf.placeholder(tf.float32, shape=(None,)+image_shape+(nr_channel,),\n",
    "                              name='data')\n",
    "        label = tf.placeholder(tf.int32, shape=(None,), name='label')\n",
    "        label_onehot = tf.one_hot(label, nr_class, dtype=tf.int32) \n",
    "        is_training = tf.placeholder(tf.bool, name='is_training') # a flag of bn（）\n",
    "        \n",
    "        # conv1\n",
    "        x = self._conv_layer(name='conv1', inp=data,\n",
    "                             kernel_shape=[3, 3, nr_channel, 16], stride=1,\n",
    "                             is_training=is_training) # Nx32x32x32\n",
    "        x = self._pool_layer(name='pool1', inp=x, ksize=2, stride=2, mode='MAX') # Nx16x16x16\n",
    "\n",
    "        # conv2\n",
    "        x = self._conv_layer(name='conv2a', inp=x, kernel_shape=[3, 3, 16, 32],\n",
    "                             stride=1, is_training=is_training)\n",
    "        x = self._conv_layer(name='conv2b', inp=x, kernel_shape=[3, 3, 32, 32],\n",
    "                             stride=1, is_training=is_training)\n",
    "        x = self._pool_layer(name='pool2', inp=x, ksize=2, stride=2, mode='MAX') # Nx8x8x32\n",
    "        \n",
    "        # conv3\n",
    "        x = self._conv_layer(name='conv3a', inp=x, kernel_shape=[3, 3, 32, 64],\n",
    "                             stride=1, is_training=is_training)\n",
    "        x = self._conv_layer(name='conv3b', inp=x, kernel_shape=[3, 3, 64, 64],\n",
    "                             stride=1, is_training=is_training)\n",
    "        x = self._pool_layer(name='pool3', inp=x, ksize=2, stride=2, mode='MAX') # Nx4x4x64\n",
    "\n",
    "        # conv4\n",
    "        x = self._conv_layer(name='conv4a', inp=x, kernel_shape=[3, 3, 64, 128],\n",
    "                             stride=1, is_training=is_training)\n",
    "        x = self._conv_layer(name='conv4b', inp=x, kernel_shape=[3, 3, 128, 128],\n",
    "                             stride=1, is_training=is_training)\n",
    "        x = self._pool_layer(name='pool4', inp=x, ksize=4, stride=4, mode='AVG') # Nx1x1x128\n",
    "        \n",
    "        # fc\n",
    "        logits = self._fc_layer(name='fc1', inp=x, units=nr_class, dropout=0)\n",
    "        \n",
    "        placeholders = {\n",
    "            'data': data,\n",
    "            'label': label,\n",
    "            'is_training': is_training,\n",
    "        }\n",
    "        return placeholders, label_onehot, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./obj/train_sp.obj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0308 19:29:28.257074   424 deprecation.py:323] From D:\\Anoconda\\new\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:494: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n",
      "W0308 19:29:28.281011   424 deprecation.py:323] From <ipython-input-39-d400a89c5fab>:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data\\\\test_32x32.mat']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0308 19:29:28.878410   424 deprecation.py:323] From <ipython-input-38-5d798daefdc3>:30: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "W0308 19:29:29.282374   424 deprecation.py:323] From <ipython-input-38-5d798daefdc3>:58: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0308 19:29:29.504775   424 deprecation.py:323] From D:\\Anoconda\\new\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # 一定要设置TF的默认图\n",
    "def get_dataset_batch(ds_name):\n",
    "    '''get a batch generator of dataset'''\n",
    "    dataset = Dataset(ds_name) # 得到数据集\n",
    "    ds_gnr = dataset.load().instance_generator # 加载数据\n",
    "    ds = tf.data.Dataset.from_generator(ds_gnr, output_types=(tf.float32, tf.int32),) #创建一个迭代器\n",
    "    if ds_name == 'train':\n",
    "        ds = ds.shuffle(dataset.instances_per_epoch) #在数据集中每个epoch都进行随机洗牌\n",
    "        ds = ds.repeat(nr_epoch) #设置最高重复使用的次数\n",
    "    elif ds_name == 'test':\n",
    "        \"测试集不用shuffle\"\n",
    "        ds = ds.repeat(nr_epoch // test_interval)\n",
    "    ds = ds.batch(minibatch_size, drop_remainder=True) # 按照指定的大小batch 大小为64\n",
    "    ds_iter = ds.make_one_shot_iterator() # 创建一个迭代器：产生一次训练真实图片数据\n",
    "    sample_gnr = ds_iter.get_next() # 获取下一组数据\n",
    "    return sample_gnr, dataset\n",
    "\n",
    "# load datasets\n",
    "train_batch_gnr, train_set = get_dataset_batch(ds_name='train')\n",
    "test_batch_gnr, test_set = get_dataset_batch(ds_name='test')\n",
    "\n",
    "\n",
    "# build a compute graph\n",
    "network = Model() \n",
    "placeholders, label_onehot, logits = network.build() # 数据流动\n",
    "\n",
    "# \"\"\"softmax\n",
    "preds = tf.nn.softmax(logits) # 最后一层的激活函数\n",
    "\n",
    "\n",
    "\"\"\"non-negative-max\n",
    "relu_logits = tf.nn.relu(logits)\n",
    "preds = relu_logits / tf.reduce_sum((relu_logits),axis=1,keepdims = True)\"\"\"\n",
    "\n",
    "\"\"\"square logits\n",
    "square_logits = logits*logits\n",
    "preds = square_logits / tf.reduce_sum(square_logits,axis=1,keepdims = True)\"\"\"\n",
    "\n",
    "\"\"\"abs-max\n",
    "abs_logits = tf.abs(logits)\n",
    "preds = abs_logits / tf.reduce_sum(abs_logits, axis=1, keepdims=True)\"\"\"\n",
    "\n",
    "\"\"\"plus one-max\"\n",
    "plus_one_abs_logits = tf.abs(logits) + 1.0\n",
    "preds = plus_one_abs_logits / tf.reduce_sum(plus_one_abs_logits, axis=1, keepdims=True)\"\"\"\n",
    "\n",
    "\n",
    "loss_reg = tf.add_n(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)) # 正则化\n",
    "loss = tf.losses.softmax_cross_entropy(label_onehot, logits) + loss_reg # 计算损失 + 正则化系数\n",
    "# loss = tf.losses.mean_squared_error(label_onehot, logits) + loss_reg\n",
    "\n",
    "# set a performance metric\n",
    "correct_pred = tf.equal(tf.cast(tf.argmax(preds, 1), dtype=tf.int32),\n",
    "                        tf.cast(tf.argmax(label_onehot, 1), dtype=tf.int32))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# learn rate config\n",
    "global_steps = tf.Variable(0, trainable=False) # a cnt to record the num of minibatchs\n",
    "boundaries = [train_set.minibatchs_per_epoch*15, train_set.minibatchs_per_epoch*40] \n",
    "values = [0.01, 0.001, 0.0005]\n",
    "lr = tf.train.piecewise_constant(global_steps, boundaries, values)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(lr) # use adam as optimizer\n",
    "\n",
    "# in order to update BN in every iter, a trick in tf\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train = opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:0,10/200 loss:2.091 acc:0.141 lr:0.010\n",
      "e:0,20/200 loss:2.044 acc:0.109 lr:0.010\n",
      "e:0,30/200 loss:1.953 acc:0.078 lr:0.010\n",
      "e:0,40/200 loss:1.998 acc:0.125 lr:0.010\n",
      "e:0,50/200 loss:2.083 acc:0.109 lr:0.010\n",
      "e:0,60/200 loss:2.114 acc:0.156 lr:0.010\n",
      "e:0,70/200 loss:1.957 acc:0.312 lr:0.010\n",
      "e:0,80/200 loss:1.989 acc:0.266 lr:0.010\n",
      "e:0,90/200 loss:1.985 acc:0.188 lr:0.010\n",
      "e:0,100/200 loss:2.070 acc:0.141 lr:0.010\n",
      "e:0,110/200 loss:2.053 acc:0.172 lr:0.010\n",
      "e:0,120/200 loss:2.082 acc:0.094 lr:0.010\n",
      "e:0,130/200 loss:2.180 acc:0.250 lr:0.010\n",
      "e:0,140/200 loss:1.974 acc:0.109 lr:0.010\n",
      "e:0,150/200 loss:2.076 acc:0.094 lr:0.010\n",
      "e:0,160/200 loss:2.131 acc:0.125 lr:0.010\n",
      "e:0,170/200 loss:1.919 acc:0.172 lr:0.010\n",
      "e:0,180/200 loss:2.133 acc:0.125 lr:0.010\n",
      "e:0,190/200 loss:2.083 acc:0.203 lr:0.010\n",
      "e:0,0/200 loss:2.250 acc:0.172 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 2.149 accuracy_avg: 0.111\n",
      "************************************************\n",
      "\n",
      "e:1,10/200 loss:2.120 acc:0.172 lr:0.010\n",
      "e:1,20/200 loss:2.287 acc:0.125 lr:0.010\n",
      "e:1,30/200 loss:1.970 acc:0.188 lr:0.010\n",
      "e:1,40/200 loss:2.162 acc:0.156 lr:0.010\n",
      "e:1,50/200 loss:2.051 acc:0.141 lr:0.010\n",
      "e:1,60/200 loss:2.635 acc:0.031 lr:0.010\n",
      "e:1,70/200 loss:2.251 acc:0.234 lr:0.010\n",
      "e:1,80/200 loss:2.360 acc:0.078 lr:0.010\n",
      "e:1,90/200 loss:1.983 acc:0.234 lr:0.010\n",
      "e:1,100/200 loss:2.373 acc:0.219 lr:0.010\n",
      "e:1,110/200 loss:2.656 acc:0.031 lr:0.010\n",
      "e:1,120/200 loss:2.774 acc:0.094 lr:0.010\n",
      "e:1,130/200 loss:2.336 acc:0.188 lr:0.010\n",
      "e:1,140/200 loss:2.414 acc:0.094 lr:0.010\n",
      "e:1,150/200 loss:2.249 acc:0.109 lr:0.010\n",
      "e:1,160/200 loss:2.656 acc:0.125 lr:0.010\n",
      "e:1,170/200 loss:2.672 acc:0.062 lr:0.010\n",
      "e:1,180/200 loss:2.408 acc:0.125 lr:0.010\n",
      "e:1,190/200 loss:2.321 acc:0.109 lr:0.010\n",
      "e:1,0/200 loss:2.481 acc:0.062 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 2.149 accuracy_avg: 0.111\n",
      "************************************************\n",
      "\n",
      "e:2,10/200 loss:2.747 acc:0.031 lr:0.010\n",
      "e:2,20/200 loss:2.519 acc:0.062 lr:0.010\n",
      "e:2,30/200 loss:2.401 acc:0.047 lr:0.010\n",
      "e:2,40/200 loss:3.324 acc:0.016 lr:0.010\n",
      "e:2,50/200 loss:2.372 acc:0.125 lr:0.010\n",
      "e:2,60/200 loss:2.405 acc:0.172 lr:0.010\n",
      "e:2,70/200 loss:2.639 acc:0.172 lr:0.010\n",
      "e:2,80/200 loss:2.490 acc:0.281 lr:0.010\n",
      "e:2,90/200 loss:3.019 acc:0.141 lr:0.010\n",
      "e:2,100/200 loss:2.402 acc:0.031 lr:0.010\n",
      "e:2,110/200 loss:3.715 acc:0.203 lr:0.010\n",
      "e:2,120/200 loss:4.708 acc:0.188 lr:0.010\n",
      "e:2,130/200 loss:6.103 acc:0.141 lr:0.010\n",
      "e:2,140/200 loss:5.332 acc:0.125 lr:0.010\n",
      "e:2,150/200 loss:4.307 acc:0.125 lr:0.010\n",
      "e:2,160/200 loss:5.647 acc:0.156 lr:0.010\n",
      "e:2,170/200 loss:5.414 acc:0.172 lr:0.010\n",
      "e:2,180/200 loss:2.850 acc:0.219 lr:0.010\n",
      "e:2,190/200 loss:2.477 acc:0.172 lr:0.010\n",
      "e:2,0/200 loss:3.739 acc:0.203 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 2.149 accuracy_avg: 0.111\n",
      "************************************************\n",
      "\n",
      "e:3,10/200 loss:3.019 acc:0.141 lr:0.010\n",
      "e:3,20/200 loss:2.919 acc:0.125 lr:0.010\n",
      "e:3,30/200 loss:2.277 acc:0.188 lr:0.010\n",
      "e:3,40/200 loss:2.347 acc:0.188 lr:0.010\n",
      "e:3,50/200 loss:3.198 acc:0.094 lr:0.010\n",
      "e:3,60/200 loss:2.823 acc:0.109 lr:0.010\n",
      "e:3,70/200 loss:3.791 acc:0.062 lr:0.010\n",
      "e:3,80/200 loss:7.207 acc:0.156 lr:0.010\n",
      "e:3,90/200 loss:6.688 acc:0.125 lr:0.010\n",
      "e:3,100/200 loss:5.740 acc:0.078 lr:0.010\n",
      "e:3,110/200 loss:6.485 acc:0.156 lr:0.010\n",
      "e:3,120/200 loss:6.641 acc:0.016 lr:0.010\n",
      "e:3,130/200 loss:5.574 acc:0.109 lr:0.010\n",
      "e:3,140/200 loss:4.757 acc:0.062 lr:0.010\n",
      "e:3,150/200 loss:4.042 acc:0.156 lr:0.010\n",
      "e:3,160/200 loss:4.668 acc:0.141 lr:0.010\n",
      "e:3,170/200 loss:4.867 acc:0.125 lr:0.010\n",
      "e:3,180/200 loss:2.865 acc:0.109 lr:0.010\n",
      "e:3,190/200 loss:3.801 acc:0.031 lr:0.010\n",
      "e:3,0/200 loss:3.212 acc:0.156 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 2.149 accuracy_avg: 0.111\n",
      "************************************************\n",
      "\n",
      "e:4,10/200 loss:3.721 acc:0.156 lr:0.010\n",
      "e:4,20/200 loss:2.969 acc:0.188 lr:0.010\n",
      "e:4,30/200 loss:4.467 acc:0.219 lr:0.010\n",
      "e:4,40/200 loss:4.240 acc:0.047 lr:0.010\n",
      "e:4,50/200 loss:4.148 acc:0.062 lr:0.010\n",
      "e:4,60/200 loss:8.365 acc:0.188 lr:0.010\n",
      "e:4,70/200 loss:6.896 acc:0.141 lr:0.010\n",
      "e:4,80/200 loss:8.143 acc:0.047 lr:0.010\n",
      "e:4,90/200 loss:4.796 acc:0.219 lr:0.010\n",
      "e:4,100/200 loss:3.639 acc:0.141 lr:0.010\n",
      "e:4,110/200 loss:3.476 acc:0.141 lr:0.010\n",
      "e:4,120/200 loss:8.298 acc:0.141 lr:0.010\n",
      "e:4,130/200 loss:5.409 acc:0.125 lr:0.010\n",
      "e:4,140/200 loss:8.114 acc:0.203 lr:0.010\n",
      "e:4,150/200 loss:7.925 acc:0.031 lr:0.010\n",
      "e:4,160/200 loss:5.693 acc:0.062 lr:0.010\n",
      "e:4,170/200 loss:6.478 acc:0.172 lr:0.010\n",
      "e:4,180/200 loss:4.326 acc:0.062 lr:0.010\n",
      "e:4,190/200 loss:7.583 acc:0.125 lr:0.010\n",
      "e:4,0/200 loss:5.845 acc:0.234 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 2.149 accuracy_avg: 0.111\n",
      "************************************************\n",
      "\n",
      "e:5,10/200 loss:7.419 acc:0.078 lr:0.010\n",
      "e:5,20/200 loss:7.384 acc:0.094 lr:0.010\n",
      "e:5,30/200 loss:7.051 acc:0.109 lr:0.010\n",
      "e:5,40/200 loss:8.761 acc:0.172 lr:0.010\n",
      "e:5,50/200 loss:5.903 acc:0.172 lr:0.010\n",
      "e:5,60/200 loss:3.951 acc:0.141 lr:0.010\n",
      "e:5,70/200 loss:5.422 acc:0.125 lr:0.010\n",
      "e:5,80/200 loss:6.772 acc:0.234 lr:0.010\n",
      "e:5,90/200 loss:10.398 acc:0.078 lr:0.010\n",
      "e:5,100/200 loss:6.059 acc:0.078 lr:0.010\n",
      "e:5,110/200 loss:11.089 acc:0.016 lr:0.010\n",
      "e:5,120/200 loss:6.960 acc:0.109 lr:0.010\n",
      "e:5,130/200 loss:9.217 acc:0.078 lr:0.010\n",
      "e:5,140/200 loss:14.416 acc:0.078 lr:0.010\n",
      "e:5,150/200 loss:7.980 acc:0.109 lr:0.010\n",
      "e:5,160/200 loss:10.629 acc:0.016 lr:0.010\n",
      "e:5,170/200 loss:7.132 acc:0.062 lr:0.010\n",
      "e:5,180/200 loss:7.120 acc:0.062 lr:0.010\n",
      "e:5,190/200 loss:17.453 acc:0.094 lr:0.010\n",
      "e:5,0/200 loss:13.979 acc:0.125 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 8.538 accuracy_avg: 0.075\n",
      "************************************************\n",
      "\n",
      "e:6,10/200 loss:4.328 acc:0.062 lr:0.010\n",
      "e:6,20/200 loss:7.208 acc:0.297 lr:0.010\n",
      "e:6,30/200 loss:10.290 acc:0.141 lr:0.010\n",
      "e:6,40/200 loss:6.749 acc:0.109 lr:0.010\n",
      "e:6,50/200 loss:6.286 acc:0.109 lr:0.010\n",
      "e:6,60/200 loss:7.887 acc:0.047 lr:0.010\n",
      "e:6,70/200 loss:13.805 acc:0.125 lr:0.010\n",
      "e:6,80/200 loss:12.599 acc:0.109 lr:0.010\n",
      "e:6,90/200 loss:8.101 acc:0.125 lr:0.010\n",
      "e:6,100/200 loss:8.016 acc:0.078 lr:0.010\n",
      "e:6,110/200 loss:5.405 acc:0.094 lr:0.010\n",
      "e:6,120/200 loss:8.557 acc:0.094 lr:0.010\n",
      "e:6,130/200 loss:8.862 acc:0.000 lr:0.010\n",
      "e:6,140/200 loss:13.236 acc:0.047 lr:0.010\n",
      "e:6,150/200 loss:6.519 acc:0.156 lr:0.010\n",
      "e:6,160/200 loss:13.127 acc:0.109 lr:0.010\n",
      "e:6,170/200 loss:15.097 acc:0.203 lr:0.010\n",
      "e:6,180/200 loss:8.941 acc:0.219 lr:0.010\n",
      "e:6,190/200 loss:11.982 acc:0.109 lr:0.010\n",
      "e:6,0/200 loss:8.158 acc:0.109 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 8.538 accuracy_avg: 0.075\n",
      "************************************************\n",
      "\n",
      "e:7,10/200 loss:10.919 acc:0.031 lr:0.010\n",
      "e:7,20/200 loss:8.496 acc:0.109 lr:0.010\n",
      "e:7,30/200 loss:11.066 acc:0.078 lr:0.010\n",
      "e:7,40/200 loss:9.991 acc:0.094 lr:0.010\n",
      "e:7,50/200 loss:11.448 acc:0.266 lr:0.010\n",
      "e:7,60/200 loss:15.030 acc:0.125 lr:0.010\n",
      "e:7,70/200 loss:4.582 acc:0.156 lr:0.010\n",
      "e:7,80/200 loss:6.681 acc:0.078 lr:0.010\n",
      "e:7,90/200 loss:12.505 acc:0.031 lr:0.010\n",
      "e:7,100/200 loss:11.534 acc:0.125 lr:0.010\n",
      "e:7,110/200 loss:14.050 acc:0.031 lr:0.010\n",
      "e:7,120/200 loss:16.065 acc:0.109 lr:0.010\n",
      "e:7,130/200 loss:18.410 acc:0.031 lr:0.010\n",
      "e:7,140/200 loss:13.562 acc:0.062 lr:0.010\n",
      "e:7,150/200 loss:17.595 acc:0.078 lr:0.010\n",
      "e:7,160/200 loss:11.937 acc:0.016 lr:0.010\n",
      "e:7,170/200 loss:13.753 acc:0.125 lr:0.010\n",
      "e:7,180/200 loss:12.238 acc:0.031 lr:0.010\n",
      "e:7,190/200 loss:10.187 acc:0.109 lr:0.010\n",
      "e:7,0/200 loss:26.784 acc:0.109 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 8.538 accuracy_avg: 0.075\n",
      "************************************************\n",
      "\n",
      "e:8,10/200 loss:16.978 acc:0.125 lr:0.010\n",
      "e:8,20/200 loss:16.886 acc:0.141 lr:0.010\n",
      "e:8,30/200 loss:18.592 acc:0.094 lr:0.010\n",
      "e:8,40/200 loss:15.977 acc:0.125 lr:0.010\n",
      "e:8,50/200 loss:19.341 acc:0.078 lr:0.010\n",
      "e:8,60/200 loss:21.253 acc:0.172 lr:0.010\n",
      "e:8,70/200 loss:5.033 acc:0.156 lr:0.010\n",
      "e:8,80/200 loss:12.587 acc:0.078 lr:0.010\n",
      "e:8,90/200 loss:10.837 acc:0.172 lr:0.010\n",
      "e:8,100/200 loss:12.552 acc:0.062 lr:0.010\n",
      "e:8,110/200 loss:13.496 acc:0.141 lr:0.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:8,120/200 loss:11.143 acc:0.094 lr:0.010\n",
      "e:8,130/200 loss:12.093 acc:0.078 lr:0.010\n",
      "e:8,140/200 loss:13.916 acc:0.109 lr:0.010\n",
      "e:8,150/200 loss:17.635 acc:0.109 lr:0.010\n",
      "e:8,160/200 loss:15.645 acc:0.078 lr:0.010\n",
      "e:8,170/200 loss:11.112 acc:0.031 lr:0.010\n",
      "e:8,180/200 loss:5.144 acc:0.172 lr:0.010\n",
      "e:8,190/200 loss:13.650 acc:0.156 lr:0.010\n",
      "e:8,0/200 loss:16.787 acc:0.250 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 8.538 accuracy_avg: 0.075\n",
      "************************************************\n",
      "\n",
      "e:9,10/200 loss:12.357 acc:0.047 lr:0.010\n",
      "e:9,20/200 loss:16.073 acc:0.234 lr:0.010\n",
      "e:9,30/200 loss:16.214 acc:0.172 lr:0.010\n",
      "e:9,40/200 loss:7.223 acc:0.094 lr:0.010\n",
      "e:9,50/200 loss:11.849 acc:0.188 lr:0.010\n",
      "e:9,60/200 loss:8.628 acc:0.203 lr:0.010\n",
      "e:9,70/200 loss:9.113 acc:0.172 lr:0.010\n",
      "e:9,80/200 loss:11.247 acc:0.031 lr:0.010\n",
      "e:9,90/200 loss:14.804 acc:0.203 lr:0.010\n",
      "e:9,100/200 loss:15.597 acc:0.047 lr:0.010\n",
      "e:9,110/200 loss:12.580 acc:0.125 lr:0.010\n",
      "e:9,120/200 loss:9.400 acc:0.125 lr:0.010\n",
      "e:9,130/200 loss:9.034 acc:0.188 lr:0.010\n",
      "e:9,140/200 loss:9.283 acc:0.000 lr:0.010\n",
      "e:9,150/200 loss:10.913 acc:0.188 lr:0.010\n",
      "e:9,160/200 loss:18.174 acc:0.094 lr:0.010\n",
      "e:9,170/200 loss:18.080 acc:0.109 lr:0.010\n",
      "e:9,180/200 loss:23.219 acc:0.109 lr:0.010\n",
      "e:9,190/200 loss:22.154 acc:0.141 lr:0.010\n",
      "e:9,0/200 loss:19.298 acc:0.156 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 8.538 accuracy_avg: 0.075\n",
      "************************************************\n",
      "\n",
      "e:10,10/200 loss:18.105 acc:0.125 lr:0.010\n",
      "e:10,20/200 loss:24.638 acc:0.047 lr:0.010\n",
      "e:10,30/200 loss:21.240 acc:0.062 lr:0.010\n",
      "e:10,40/200 loss:15.041 acc:0.078 lr:0.010\n",
      "e:10,50/200 loss:11.306 acc:0.047 lr:0.010\n",
      "e:10,60/200 loss:8.495 acc:0.109 lr:0.010\n",
      "e:10,70/200 loss:7.734 acc:0.109 lr:0.010\n",
      "e:10,80/200 loss:15.163 acc:0.094 lr:0.010\n",
      "e:10,90/200 loss:13.161 acc:0.109 lr:0.010\n",
      "e:10,100/200 loss:23.923 acc:0.047 lr:0.010\n",
      "e:10,110/200 loss:13.954 acc:0.094 lr:0.010\n",
      "e:10,120/200 loss:22.635 acc:0.094 lr:0.010\n",
      "e:10,130/200 loss:10.107 acc:0.188 lr:0.010\n",
      "e:10,140/200 loss:10.878 acc:0.125 lr:0.010\n",
      "e:10,150/200 loss:14.696 acc:0.109 lr:0.010\n",
      "e:10,160/200 loss:9.871 acc:0.078 lr:0.010\n",
      "e:10,170/200 loss:14.082 acc:0.109 lr:0.010\n",
      "e:10,180/200 loss:17.972 acc:0.109 lr:0.010\n",
      "e:10,190/200 loss:21.947 acc:0.141 lr:0.010\n",
      "e:10,0/200 loss:14.302 acc:0.062 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 18.263 accuracy_avg: 0.062\n",
      "************************************************\n",
      "\n",
      "e:11,10/200 loss:9.415 acc:0.125 lr:0.010\n",
      "e:11,20/200 loss:10.172 acc:0.094 lr:0.010\n",
      "e:11,30/200 loss:11.272 acc:0.219 lr:0.010\n",
      "e:11,40/200 loss:11.578 acc:0.062 lr:0.010\n",
      "e:11,50/200 loss:25.187 acc:0.016 lr:0.010\n",
      "e:11,60/200 loss:23.166 acc:0.078 lr:0.010\n",
      "e:11,70/200 loss:28.264 acc:0.094 lr:0.010\n",
      "e:11,80/200 loss:21.952 acc:0.156 lr:0.010\n",
      "e:11,90/200 loss:10.423 acc:0.078 lr:0.010\n",
      "e:11,100/200 loss:15.894 acc:0.109 lr:0.010\n",
      "e:11,110/200 loss:14.200 acc:0.094 lr:0.010\n",
      "e:11,120/200 loss:16.384 acc:0.062 lr:0.010\n",
      "e:11,130/200 loss:28.321 acc:0.156 lr:0.010\n",
      "e:11,140/200 loss:27.776 acc:0.031 lr:0.010\n",
      "e:11,150/200 loss:12.664 acc:0.094 lr:0.010\n",
      "e:11,160/200 loss:17.426 acc:0.188 lr:0.010\n",
      "e:11,170/200 loss:12.932 acc:0.266 lr:0.010\n",
      "e:11,180/200 loss:17.839 acc:0.016 lr:0.010\n",
      "e:11,190/200 loss:13.331 acc:0.109 lr:0.010\n",
      "e:11,0/200 loss:12.383 acc:0.156 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 18.263 accuracy_avg: 0.062\n",
      "************************************************\n",
      "\n",
      "e:12,10/200 loss:17.761 acc:0.141 lr:0.010\n",
      "e:12,20/200 loss:19.094 acc:0.219 lr:0.010\n",
      "e:12,30/200 loss:25.217 acc:0.141 lr:0.010\n",
      "e:12,40/200 loss:13.677 acc:0.203 lr:0.010\n",
      "e:12,50/200 loss:23.097 acc:0.156 lr:0.010\n",
      "e:12,60/200 loss:23.024 acc:0.047 lr:0.010\n",
      "e:12,70/200 loss:20.193 acc:0.109 lr:0.010\n",
      "e:12,80/200 loss:16.786 acc:0.031 lr:0.010\n",
      "e:12,90/200 loss:22.291 acc:0.156 lr:0.010\n",
      "e:12,100/200 loss:22.441 acc:0.078 lr:0.010\n",
      "e:12,110/200 loss:33.480 acc:0.016 lr:0.010\n",
      "e:12,120/200 loss:22.147 acc:0.094 lr:0.010\n",
      "e:12,130/200 loss:18.533 acc:0.125 lr:0.010\n",
      "e:12,140/200 loss:15.078 acc:0.125 lr:0.010\n",
      "e:12,150/200 loss:17.326 acc:0.062 lr:0.010\n",
      "e:12,160/200 loss:18.601 acc:0.156 lr:0.010\n",
      "e:12,170/200 loss:17.032 acc:0.109 lr:0.010\n",
      "e:12,180/200 loss:26.804 acc:0.141 lr:0.010\n",
      "e:12,190/200 loss:29.640 acc:0.078 lr:0.010\n",
      "e:12,0/200 loss:27.004 acc:0.094 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 18.263 accuracy_avg: 0.062\n",
      "************************************************\n",
      "\n",
      "e:13,10/200 loss:13.074 acc:0.203 lr:0.010\n",
      "e:13,20/200 loss:20.554 acc:0.109 lr:0.010\n",
      "e:13,30/200 loss:19.927 acc:0.109 lr:0.010\n",
      "e:13,40/200 loss:17.638 acc:0.078 lr:0.010\n",
      "e:13,50/200 loss:28.225 acc:0.094 lr:0.010\n",
      "e:13,60/200 loss:17.654 acc:0.203 lr:0.010\n",
      "e:13,70/200 loss:20.384 acc:0.109 lr:0.010\n",
      "e:13,80/200 loss:12.482 acc:0.031 lr:0.010\n",
      "e:13,90/200 loss:6.071 acc:0.062 lr:0.010\n",
      "e:13,100/200 loss:14.757 acc:0.047 lr:0.010\n",
      "e:13,110/200 loss:12.125 acc:0.094 lr:0.010\n",
      "e:13,120/200 loss:18.076 acc:0.094 lr:0.010\n",
      "e:13,130/200 loss:33.122 acc:0.094 lr:0.010\n",
      "e:13,140/200 loss:26.475 acc:0.094 lr:0.010\n",
      "e:13,150/200 loss:24.484 acc:0.141 lr:0.010\n",
      "e:13,160/200 loss:33.717 acc:0.109 lr:0.010\n",
      "e:13,170/200 loss:28.735 acc:0.062 lr:0.010\n",
      "e:13,180/200 loss:36.751 acc:0.141 lr:0.010\n",
      "e:13,190/200 loss:17.790 acc:0.203 lr:0.010\n",
      "e:13,0/200 loss:21.639 acc:0.141 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 18.263 accuracy_avg: 0.062\n",
      "************************************************\n",
      "\n",
      "e:14,10/200 loss:23.518 acc:0.125 lr:0.010\n",
      "e:14,20/200 loss:26.767 acc:0.141 lr:0.010\n",
      "e:14,30/200 loss:20.851 acc:0.078 lr:0.010\n",
      "e:14,40/200 loss:20.228 acc:0.047 lr:0.010\n",
      "e:14,50/200 loss:32.132 acc:0.125 lr:0.010\n",
      "e:14,60/200 loss:20.400 acc:0.188 lr:0.010\n",
      "e:14,70/200 loss:32.228 acc:0.156 lr:0.010\n",
      "e:14,80/200 loss:29.699 acc:0.078 lr:0.010\n",
      "e:14,90/200 loss:23.448 acc:0.094 lr:0.010\n",
      "e:14,100/200 loss:24.857 acc:0.156 lr:0.010\n",
      "e:14,110/200 loss:32.132 acc:0.094 lr:0.010\n",
      "e:14,120/200 loss:27.530 acc:0.031 lr:0.010\n",
      "e:14,130/200 loss:27.490 acc:0.047 lr:0.010\n",
      "e:14,140/200 loss:13.180 acc:0.047 lr:0.010\n",
      "e:14,150/200 loss:14.917 acc:0.062 lr:0.010\n",
      "e:14,160/200 loss:19.168 acc:0.109 lr:0.010\n",
      "e:14,170/200 loss:23.357 acc:0.109 lr:0.010\n",
      "e:14,180/200 loss:16.148 acc:0.062 lr:0.010\n",
      "e:14,190/200 loss:11.821 acc:0.047 lr:0.010\n",
      "e:14,0/200 loss:20.635 acc:0.031 lr:0.010\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 18.263 accuracy_avg: 0.062\n",
      "************************************************\n",
      "\n",
      "e:15,10/200 loss:8.854 acc:0.219 lr:0.001\n",
      "e:15,20/200 loss:3.782 acc:0.047 lr:0.001\n",
      "e:15,30/200 loss:3.267 acc:0.125 lr:0.001\n",
      "e:15,40/200 loss:2.713 acc:0.094 lr:0.001\n",
      "e:15,50/200 loss:2.631 acc:0.109 lr:0.001\n",
      "e:15,60/200 loss:2.055 acc:0.125 lr:0.001\n",
      "e:15,70/200 loss:2.011 acc:0.203 lr:0.001\n",
      "e:15,80/200 loss:2.083 acc:0.172 lr:0.001\n",
      "e:15,90/200 loss:2.200 acc:0.219 lr:0.001\n",
      "e:15,100/200 loss:2.179 acc:0.062 lr:0.001\n",
      "e:15,110/200 loss:2.078 acc:0.141 lr:0.001\n",
      "e:15,120/200 loss:2.125 acc:0.172 lr:0.001\n",
      "e:15,130/200 loss:2.172 acc:0.094 lr:0.001\n",
      "e:15,140/200 loss:2.114 acc:0.188 lr:0.001\n",
      "e:15,150/200 loss:1.947 acc:0.266 lr:0.001\n",
      "e:15,160/200 loss:2.184 acc:0.188 lr:0.001\n",
      "e:15,170/200 loss:1.953 acc:0.141 lr:0.001\n",
      "e:15,180/200 loss:1.847 acc:0.203 lr:0.001\n",
      "e:15,190/200 loss:2.256 acc:0.156 lr:0.001\n",
      "e:15,0/200 loss:2.030 acc:0.188 lr:0.001\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 16.227 accuracy_avg: 0.157\n",
      "************************************************\n",
      "\n",
      "e:16,10/200 loss:2.021 acc:0.094 lr:0.001\n",
      "e:16,20/200 loss:2.014 acc:0.141 lr:0.001\n",
      "e:16,30/200 loss:2.179 acc:0.141 lr:0.001\n",
      "e:16,40/200 loss:2.113 acc:0.141 lr:0.001\n",
      "e:16,50/200 loss:2.053 acc:0.219 lr:0.001\n",
      "e:16,60/200 loss:2.125 acc:0.141 lr:0.001\n",
      "e:16,70/200 loss:2.124 acc:0.141 lr:0.001\n",
      "e:16,80/200 loss:2.048 acc:0.219 lr:0.001\n",
      "e:16,90/200 loss:1.995 acc:0.172 lr:0.001\n",
      "e:16,100/200 loss:2.081 acc:0.141 lr:0.001\n",
      "e:16,110/200 loss:2.038 acc:0.156 lr:0.001\n",
      "e:16,120/200 loss:2.163 acc:0.156 lr:0.001\n",
      "e:16,130/200 loss:1.984 acc:0.125 lr:0.001\n",
      "e:16,140/200 loss:2.066 acc:0.125 lr:0.001\n",
      "e:16,150/200 loss:2.112 acc:0.172 lr:0.001\n",
      "e:16,160/200 loss:2.064 acc:0.172 lr:0.001\n",
      "e:16,170/200 loss:2.042 acc:0.234 lr:0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:16,180/200 loss:2.175 acc:0.188 lr:0.001\n",
      "e:16,190/200 loss:2.013 acc:0.188 lr:0.001\n",
      "e:16,0/200 loss:2.052 acc:0.125 lr:0.001\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 16.227 accuracy_avg: 0.157\n",
      "************************************************\n",
      "\n",
      "e:17,10/200 loss:2.130 acc:0.078 lr:0.001\n",
      "e:17,20/200 loss:1.913 acc:0.172 lr:0.001\n",
      "e:17,30/200 loss:2.095 acc:0.156 lr:0.001\n",
      "e:17,40/200 loss:2.205 acc:0.172 lr:0.001\n",
      "e:17,50/200 loss:2.205 acc:0.125 lr:0.001\n",
      "e:17,60/200 loss:2.046 acc:0.172 lr:0.001\n",
      "e:17,70/200 loss:2.060 acc:0.078 lr:0.001\n",
      "e:17,80/200 loss:2.155 acc:0.047 lr:0.001\n",
      "e:17,90/200 loss:2.007 acc:0.156 lr:0.001\n",
      "e:17,100/200 loss:2.167 acc:0.094 lr:0.001\n",
      "e:17,110/200 loss:2.101 acc:0.172 lr:0.001\n",
      "e:17,120/200 loss:2.050 acc:0.125 lr:0.001\n",
      "e:17,130/200 loss:2.031 acc:0.156 lr:0.001\n",
      "e:17,140/200 loss:2.160 acc:0.109 lr:0.001\n",
      "e:17,150/200 loss:2.142 acc:0.188 lr:0.001\n",
      "e:17,160/200 loss:2.249 acc:0.188 lr:0.001\n",
      "e:17,170/200 loss:2.106 acc:0.062 lr:0.001\n",
      "e:17,180/200 loss:2.017 acc:0.297 lr:0.001\n",
      "e:17,190/200 loss:2.062 acc:0.234 lr:0.001\n",
      "e:17,0/200 loss:2.101 acc:0.172 lr:0.001\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 16.227 accuracy_avg: 0.157\n",
      "************************************************\n",
      "\n",
      "e:18,10/200 loss:2.134 acc:0.109 lr:0.001\n",
      "e:18,20/200 loss:2.073 acc:0.094 lr:0.001\n",
      "e:18,30/200 loss:2.155 acc:0.141 lr:0.001\n",
      "e:18,40/200 loss:2.115 acc:0.078 lr:0.001\n",
      "e:18,50/200 loss:2.142 acc:0.172 lr:0.001\n",
      "e:18,60/200 loss:2.074 acc:0.203 lr:0.001\n",
      "e:18,70/200 loss:2.071 acc:0.266 lr:0.001\n",
      "e:18,80/200 loss:2.241 acc:0.062 lr:0.001\n",
      "e:18,90/200 loss:2.003 acc:0.172 lr:0.001\n",
      "e:18,100/200 loss:2.009 acc:0.141 lr:0.001\n",
      "e:18,110/200 loss:2.128 acc:0.172 lr:0.001\n",
      "e:18,120/200 loss:1.923 acc:0.156 lr:0.001\n",
      "e:18,130/200 loss:2.139 acc:0.078 lr:0.001\n",
      "e:18,140/200 loss:2.033 acc:0.156 lr:0.001\n",
      "e:18,150/200 loss:2.112 acc:0.109 lr:0.001\n",
      "e:18,160/200 loss:2.225 acc:0.109 lr:0.001\n",
      "e:18,170/200 loss:1.938 acc:0.094 lr:0.001\n",
      "e:18,180/200 loss:2.355 acc:0.031 lr:0.001\n",
      "e:18,190/200 loss:2.006 acc:0.188 lr:0.001\n",
      "e:18,0/200 loss:1.932 acc:0.172 lr:0.001\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 16.227 accuracy_avg: 0.157\n",
      "************************************************\n",
      "\n",
      "e:19,10/200 loss:2.154 acc:0.203 lr:0.001\n",
      "e:19,20/200 loss:2.028 acc:0.188 lr:0.001\n",
      "e:19,30/200 loss:2.063 acc:0.109 lr:0.001\n",
      "e:19,40/200 loss:2.171 acc:0.141 lr:0.001\n",
      "e:19,50/200 loss:2.231 acc:0.094 lr:0.001\n",
      "e:19,60/200 loss:2.245 acc:0.094 lr:0.001\n",
      "e:19,70/200 loss:1.939 acc:0.156 lr:0.001\n",
      "e:19,80/200 loss:1.943 acc:0.125 lr:0.001\n",
      "e:19,90/200 loss:1.936 acc:0.125 lr:0.001\n",
      "e:19,100/200 loss:1.916 acc:0.188 lr:0.001\n",
      "e:19,110/200 loss:2.082 acc:0.156 lr:0.001\n",
      "e:19,120/200 loss:2.283 acc:0.172 lr:0.001\n",
      "e:19,130/200 loss:1.924 acc:0.141 lr:0.001\n",
      "e:19,140/200 loss:2.043 acc:0.078 lr:0.001\n",
      "e:19,150/200 loss:2.156 acc:0.062 lr:0.001\n",
      "e:19,160/200 loss:2.313 acc:0.125 lr:0.001\n",
      "e:19,170/200 loss:2.212 acc:0.094 lr:0.001\n",
      "e:19,180/200 loss:2.061 acc:0.156 lr:0.001\n",
      "e:19,190/200 loss:2.250 acc:0.047 lr:0.001\n",
      "e:19,0/200 loss:2.061 acc:0.078 lr:0.001\n",
      "\n",
      "**************Validation results****************\n",
      "loss_avg: 16.227 accuracy_avg: 0.157\n",
      "************************************************\n",
      "\n",
      "trainning is done\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(12345)\n",
    "global_cnt = 0\n",
    "\n",
    "testLoss, testAcc = [], []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(nr_epoch): # 整个数据集通过网络的次数\n",
    "        for _ in range(train_set.minibatchs_per_epoch):#  迭代200次\n",
    "            global_cnt += 1\n",
    "            try:\n",
    "                images,labels = sess.run(train_batch_gnr) #每次使用64张图片\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "            fd = {\n",
    "                placeholders['data']:images,\n",
    "                placeholders['label']:labels,\n",
    "                global_steps: global_cnt,\n",
    "                placeholders['is_training']:True,\n",
    "            }\n",
    "            _, loss_v, acc_v,lr_v = sess.run([train,loss,accuracy,lr],feed_dict = fd)\n",
    "            \n",
    "            if(global_cnt % show_interval == 0):\n",
    "                print(\n",
    "                \"e:{},{}/{}\".format(e, global_cnt % train_set.minibatchs_per_epoch,\n",
    "                                   train_set.minibatchs_per_epoch),\n",
    "                \"loss:{:.3f}\".format(loss_v),\n",
    "                \"acc:{:.3f}\".format(acc_v),\n",
    "                \"lr:{:.3f}\".format(lr_v))\n",
    "                \n",
    "        if e % test_interval == 0: # test_interval = 5\n",
    "            loss_sum, acc_sum = 0, 0\n",
    "            for i in range(test_set.minibatchs_per_epoch):# test_per_epoch\n",
    "                images,labels = sess.run(test_batch_gnr)\n",
    "                fd = {\n",
    "                    placeholders['data']:images,\n",
    "                    placeholders['label']:labels,\n",
    "                    global_steps: global_cnt,\n",
    "                    placeholders['is_training']:False,\n",
    "                }\n",
    "                preds_v, loss_v, acc_v = sess.run([preds,loss,accuracy],feed_dict=fd)\n",
    "                testLoss.append(loss_v)\n",
    "                testAcc.append(acc_v)\n",
    "                loss_sum += loss_v\n",
    "                acc_sum += acc_v\n",
    "        print(\"\\n**************Validation results****************\")\n",
    "        print('loss_avg: {:.3f}'.format(loss_sum / test_set.minibatchs_per_epoch),\n",
    "              'accuracy_avg: {:.3f}'.format(acc_sum / test_set.minibatchs_per_epoch))\n",
    "        print(\"************************************************\\n\") \n",
    "print('trainning is done')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump sucess\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = open('./add_saltpep.obj','wb')\n",
    "pickle.dump(testLoss,f)\n",
    "pickle.dump(testAcc,f)\n",
    "f.close()\n",
    "print(\"dump sucess\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "800\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open('./add_saltpep.obj','rb')\n",
    "testLoss = pickle.load(f)\n",
    "testAcc = pickle.load(f)\n",
    "\n",
    "print(len(testLoss))\n",
    "print(len(testAcc))\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plt.plot(range(800),testLoss)\n",
    "plt.title('Loss')\n",
    "plt.subplot(212)\n",
    "plt.plot(range(800),testAcc)\n",
    "plt.title('Acc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
