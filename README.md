## 01-svhn

### softmax的代替方案：

全部都可以替代softmax。

#### Loss和Acc对比图

![img](./IMG\img.PNG)

#### 实际效果对比

![res](E:\Junior Year\大三下\深度学习\Assignment\MyWorkPlace\01-svhn\Home Work\IMG\res.PNG)



### 将交叉熵替换为欧式距离

结果展示：

![lossFunc](./IMG\lossFunc.PNG)

#### 分析

- 损失函数MSE计算出的损失值小于1，交叉熵计算出的损失值大于1.
- 在验证集上进行准确率评估，交叉熵在训练开始阶段准确率高于MSE，随着模型学习到的特征越来越多之后，两者的准确率很接近。

### 尝试不同的正则化方法：L1 L2

#### 分析图

![regularize](./IMG\regularize.PNG)

#### 分析表

|      | l1-Loss  | l1-Acc   | l2-Loss  | l2-Acc   |
| ---- | -------- | -------- | -------- | -------- |
| min  | 0.022987 | 0.109375 | 0.016761 | 0.078125 |
| max  | 2.920048 | 1.000000 | 3.305678 | 1.000000 |
| mean | 0.927374 | 0.698145 | 0.972368 | 0.701621 |
| std  | 0.818694 | 0.271276 | 0.983457 | 0.299828 |

- 总体来说，l1表现出来的训练效果要比l2更加稳定。
- 这与l1的公式有关，它控制神经网络形成的高维函数尽可能的接近一个线性函数，因此相对于l2的二次函数波动较小。

### 数据集的大小对测试的影响

分别将trainSet的大小设置为ALL -> 30000 -> 10000进行实验，效果展示如下：

#### 损失函数统计性质

```
+-------+----------+----------+----------+
| value | all-loss | 3w-loss  | 1w-loss  |
+-------+----------+----------+----------+
|  min  | 0.016761 | 0.019735 | 0.026673 |
|  max  | 3.305678 | 2.973796 | 2.649049 |
|  mean | 0.972368 | 0.925272 | 1.030048 |
|  std  | 0.983457 | 0.864699 | 0.806347 |
+-------+----------+----------+----------+
```

##### 图表分析

![sizeAcc](./IMG\sizeLoss.PNG)



#### 准确率统计性质

```
+-------+----------+----------+----------+
| value | all-acc  |  3w-acc  |  1w-acc  |
+-------+----------+----------+----------+
|  min  | 0.078125 | 0.156250 | 0.078125 |
|  max  | 1.000000 | 1.000000 | 0.984375 |
|  mean | 0.701621 | 0.737051 | 0.689707 |
|  std  | 0.299828 | 0.231252 | 0.291815 |
+-------+----------+----------+----------+
```

##### 图表分析

![sizeAcc](./IMG\sizeAcc.PNG)

### 数据分布对测试效果的影响

- 限制标签为【0，8，9】的训练集数量为500
- 限制标签为【6，7，8，9，0】的训练集数量为1000
- 限制标签为【1，2，3，4，5】的训练集数量为6000

#### LOSS

```
+-------+------------+------------+------------+
| value | distA-loss | distB-loss | distC-loss |
+-------+------------+------------+------------+
|  min  |  0.017711  |  0.026269  |  0.035808  |
|  max  |  2.542600  |  2.741484  |  2.379140  |
|  mean |  0.841412  |  0.877500  |  0.844383  |
|  std  |  0.807004  |  0.876509  |  0.820668  |
+-------+------------+------------+------------+
```

![distLoss](./IMG\distLoss.PNG)



#### ACC

```
+-------+-----------+-----------+-----------+
| value | distA-acc | distB-acc | distC-acc |
+-------+-----------+-----------+-----------+
|  min  |  0.078125 |  0.046875 |  0.062500 |
|  max  |  1.000000 |  1.000000 |  1.000000 |
|  mean |  0.711035 |  0.703438 |  0.708320 |
|  std  |  0.306160 |  0.317064 |  0.309499 |
+-------+-----------+-----------+-----------+
```

![distAcc](./IMG\distAcc.PNG)

### 数据增强

#### 像素颜色反转

> color inversion: sets a pixel value from v to 255-v. 

![colorInv](./IMG\colorInv.PNG)

#### 实验效果对比

>  invert pixels from each channel individually with a probability

![color](./IMG\color.PNG)

#### 仿射变换

仿射变换通过一系列的原子变换复合实现，具体包括：平移，缩放，旋转，翻转，错切。

> - cv2.warpAffine(原始图像，移动矩阵M，变换的图像大小)。如果这个大小与原图像大小不同，cv2可以自动插值统一图像大小。
> - cv2.resize() 调整图像大小
> - cv2.getRoatationMatrix2D(旋转中心，旋转角度，旋转后图像的缩放比例) 实现图像旋转
> - cv2.warpPerspective(img,M,(200,200)) 图像的透视，参数同warpAffine

图像的旋转加上拉升就是图像仿射变换，仿射变化也是需要一个M矩阵就可以，但是由于仿射变换比较复杂，一般直接找很难找到这个矩阵，opencv提供了根据变换前后三个点的对应关系来自动求解M。这个函数是
M=cv2.getAffineTransform(pos1,pos2),其中两个位置就是变换前后的对应位置关系。输 出的就是仿射矩阵M。然后在使用函数cv2.warpAffine()。

#### 实验效果



#### 加入椒盐噪声

> 椒盐噪声也称为脉冲噪声，是图像中经常见到的一种噪声，它是一种随机出现的白点或者黑点，可能是亮的区域有黑色像素或是在暗的区域有白色像素（或是两者皆有）。

#### 效果图

![ppSalt](./IMG\ppSalt.PNG)

- 由于图像的大小是32*32，因此增加了椒盐噪声之后，几乎无法辨别图像，所以准确度很低。

![saltPep](E:\Junior Year\大三下\深度学习\Assignment\MyWorkPlace\01-svhn\Home Work\IMG\saltPep.PNG)

### 加入MIXUP --- 数据增广方法

> 来自论文：[*mixup：BEYOND EMPIRICAL RISK MINIMIZATION*](../Paper/Mixed.pdf)

经典机器学习理论告诉我们，只要学习机（如神经网络）的规模不随着训练数据数量的增加而增加，那么经验风险最小化（ERM）的收敛性就是可以得到保证的。其中，学习机的规模由参数数量，或其VC复杂度来衡量。
这一矛盾挑战了ERM方法在当前神经网络训练中的适应性。

- 一方面，即使在强正则化情况下，或是在标签随机分配的分类问题中，ERM 也允许大规模神经网络去记忆（而不是泛化）训练数据。

- 另一方面，神经网络使用ERM 方法训练后，在训练分布之外的样本（对抗样本）上验证时会极大地改变预测结果。

这一证据表明，在测试分布与训练数据略有不同时，ERM 方法已不具有良好的解释和泛化性能。
因而，数据增强方法（Simard et al., 1998），在简单但不同的样本中去训练数据以及 Vicinal Risk Minimization( VRM)领域风险最小化原则被提出。在VRM中，需要专业知识描述训练数据中每个样本的邻域，从而可以从训练样本邻域中提取附加的虚拟样本以扩充对训练分布的支持。数据增强可以提高泛化能力，但这一过程依赖于数据集，而且需要专家知识。其次，数据增强假定领域内样本都是同一类，且没有对不同类不同样本之间领域关系进行建模。

#### 公式

$$
\widetilde{x} = \lambda{x} + (1-\lambda{x})\\
\widetilde{y} = \lambda{y} + (1-\lambda{y})
$$







